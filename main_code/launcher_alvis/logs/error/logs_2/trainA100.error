+ cd ../
+ module purge
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_sh_dbg=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for Lmod's output
Shell debugging restarted
+ unset __lmod_sh_dbg
+ return 0
+ module load TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_sh_dbg=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for Lmod's output
Shell debugging restarted
+ unset __lmod_sh_dbg
+ return 0
+ srun python main_CNN.py
2024-04-25 18:11:51.794474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38423 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:31:00.0, compute capability: 8.0
2024-04-25 18:11:51.795026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38423 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:4b:00.0, compute capability: 8.0
2024-04-25 18:11:51.795429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38423 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:ca:00.0, compute capability: 8.0
2024-04-25 18:11:51.795927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 38423 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:e3:00.0, compute capability: 8.0
2024-04-25 18:11:51.799517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:worker/replica:0/task:1/device:GPU:0 with 38423 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:31:00.0, compute capability: 8.0
2024-04-25 18:11:51.799688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:worker/replica:0/task:1/device:GPU:1 with 38423 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:4b:00.0, compute capability: 8.0
2024-04-25 18:11:51.799850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:worker/replica:0/task:1/device:GPU:2 with 38423 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:ca:00.0, compute capability: 8.0
2024-04-25 18:11:51.800010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:worker/replica:0/task:1/device:GPU:3 with 38423 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:e3:00.0, compute capability: 8.0
2024-04-25 18:11:51.803165: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> alvis4-19:8888, 1 -> alvis4-23:8888}
2024-04-25 18:11:51.803312: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://alvis4-23:8888
2024-04-25 18:11:52.553201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38423 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:31:00.0, compute capability: 8.0
2024-04-25 18:11:52.553743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38423 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:4b:00.0, compute capability: 8.0
2024-04-25 18:11:52.554124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38423 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:ca:00.0, compute capability: 8.0
2024-04-25 18:11:52.554491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 38423 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:e3:00.0, compute capability: 8.0
2024-04-25 18:11:52.558253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:worker/replica:0/task:0/device:GPU:0 with 38423 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:31:00.0, compute capability: 8.0
2024-04-25 18:11:52.558421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:worker/replica:0/task:0/device:GPU:1 with 38423 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:4b:00.0, compute capability: 8.0
2024-04-25 18:11:52.558617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:worker/replica:0/task:0/device:GPU:2 with 38423 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:ca:00.0, compute capability: 8.0
2024-04-25 18:11:52.559082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:worker/replica:0/task:0/device:GPU:3 with 38423 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:e3:00.0, compute capability: 8.0
2024-04-25 18:11:52.562147: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> alvis4-19:8888, 1 -> alvis4-23:8888}
2024-04-25 18:11:52.562287: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://alvis4-19:8888
2024-04-25 18:21:20.766346: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202
2024-04-25 18:21:23.539786: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202
2024-04-25 18:21:28.992956: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202
2024-04-25 18:21:30.762807: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202
2024-04-25 18:21:32.957737: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202
2024-04-25 18:21:36.640905: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202
2024-04-25 18:21:40.065188: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202
2024-04-25 18:21:44.622405: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202
Exception: Error reading SSH protocol banner
Traceback (most recent call last):
  File "/apps/Arch/software/Python/3.9.6-GCCcore-11.2.0/lib/python3.9/site-packages/paramiko/transport.py", line 2211, in _check_banner
    buf = self.packetizer.readline(timeout)
  File "/apps/Arch/software/Python/3.9.6-GCCcore-11.2.0/lib/python3.9/site-packages/paramiko/packet.py", line 380, in readline
    buf += self._read_timeout(timeout)
  File "/apps/Arch/software/Python/3.9.6-GCCcore-11.2.0/lib/python3.9/site-packages/paramiko/packet.py", line 609, in _read_timeout
    raise EOFError()
EOFError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/apps/Arch/software/Python/3.9.6-GCCcore-11.2.0/lib/python3.9/site-packages/paramiko/transport.py", line 2039, in run
    self._check_banner()
  File "/apps/Arch/software/Python/3.9.6-GCCcore-11.2.0/lib/python3.9/site-packages/paramiko/transport.py", line 2215, in _check_banner
    raise SSHException(
paramiko.ssh_exception.SSHException: Error reading SSH protocol banner

Traceback (most recent call last):
  File "/apps/Arch/software/Python/3.9.6-GCCcore-11.2.0/lib/python3.9/site-packages/paramiko/transport.py", line 2211, in _check_banner
    buf = self.packetizer.readline(timeout)
  File "/apps/Arch/software/Python/3.9.6-GCCcore-11.2.0/lib/python3.9/site-packages/paramiko/packet.py", line 380, in readline
    buf += self._read_timeout(timeout)
  File "/apps/Arch/software/Python/3.9.6-GCCcore-11.2.0/lib/python3.9/site-packages/paramiko/packet.py", line 609, in _read_timeout
    raise EOFError()
EOFError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mimer/NOBACKUP/groups/deepmechalvis/andres/XAI_TurbulentChannel_optimized/main_code/main_CNN.py", line 211, in <module>
    Unet.train_model()
  File "/mimer/NOBACKUP/groups/deepmechalvis/andres/XAI_TurbulentChannel_optimized/main_code/py_bin/py_class/ann_config.py", line 599, in train_model
    data_trainvali = read_data_tf(data_in=data_tensor)
  File "/mimer/NOBACKUP/groups/deepmechalvis/andres/XAI_TurbulentChannel_optimized/main_code/py_bin/py_functions/trainvali_data.py", line 363, in read_data_tf
    data_readtf   = _read_datatf_function(data_in={"index":interval[len_train:],"folder_tf":folder_tf,
  File "/mimer/NOBACKUP/groups/deepmechalvis/andres/XAI_TurbulentChannel_optimized/main_code/py_bin/py_functions/trainvali_data.py", line 102, in _read_datatf_function
    read_from_server(data_in={"remotedir":folder_savetf_ii,"localdir":folder_temp_ii,"server":ssh_server,
  File "/mimer/NOBACKUP/groups/deepmechalvis/andres/XAI_TurbulentChannel_optimized/main_code/py_bin/py_remote/read_remote.py", line 111, in read_from_server
    client.connect(server,username=username,password=password)
  File "/apps/Arch/software/Python/3.9.6-GCCcore-11.2.0/lib/python3.9/site-packages/paramiko/client.py", line 406, in connect
    t.start_client(timeout=timeout)
  File "/apps/Arch/software/Python/3.9.6-GCCcore-11.2.0/lib/python3.9/site-packages/paramiko/transport.py", line 660, in start_client
    raise e
  File "/apps/Arch/software/Python/3.9.6-GCCcore-11.2.0/lib/python3.9/site-packages/paramiko/transport.py", line 2039, in run
    self._check_banner()
  File "/apps/Arch/software/Python/3.9.6-GCCcore-11.2.0/lib/python3.9/site-packages/paramiko/transport.py", line 2215, in _check_banner
    raise SSHException(
paramiko.ssh_exception.SSHException: Error reading SSH protocol banner
srun: error: alvis4-19: task 0: Exited with exit code 1
WARNING:tensorflow:/job:worker/replica:0/task:0 seems down, retrying 1/3
WARNING:tensorflow:/job:worker/replica:0/task:0 seems down, retrying 2/3
ERROR:tensorflow:Cluster check alive failed, /job:worker/replica:0/task:0 is down, aborting collectives: failed to connect to all addresses
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{"created":"@1714111558.131822748","description":"Failed to pick subchannel","file":"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc","file_line":3940,"referenced_errors":[{"created":"@1714111558.130510033","description":"failed to connect to all addresses","file":"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc","file_line":392,"grpc_status":14}]}
2024-04-26 08:05:58.131915: E tensorflow/core/common_runtime/base_collective_executor.cc:249] BaseCollectiveExecutor::StartAbort UNAVAILABLE: cluster check alive failed, /job:worker/replica:0/task:0 is down
Traceback (most recent call last):
  File "/mimer/NOBACKUP/groups/deepmechalvis/andres/XAI_TurbulentChannel_optimized/main_code/main_CNN.py", line 211, in <module>
    Unet.train_model()
  File "/mimer/NOBACKUP/groups/deepmechalvis/andres/XAI_TurbulentChannel_optimized/main_code/py_bin/py_class/ann_config.py", line 629, in train_model
    data_training = self.model.fit(train_data,batch_size=self.batch_size,verbose=2,
  File "/apps/Arch/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/utils/traceback_utils.py", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/apps/Arch/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/tensorflow/python/eager/execute.py", line 58, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnavailableError:  Collective ops is aborted by: cluster check alive failed, /job:worker/replica:0/task:0 is down
The error could be from a previous operation. Restart your program to reset.
	 [[node CollectiveReduceV2
 (defined at /apps/Arch/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/engine/training.py:866)
]] [Op:__inference_train_function_43557]

Errors may have originated from an input operation.
Input Source operations connected to node CollectiveReduceV2:
In[0] concat/concat:	
In[1] CollectiveReduceV2/group_size:	
In[2] CollectiveReduceV2/group_key:	
In[3] Placeholder:

Operation defined at: (most recent call last)
>>>   File "/mimer/NOBACKUP/groups/deepmechalvis/andres/XAI_TurbulentChannel_optimized/main_code/main_CNN.py", line 211, in <module>
>>>     Unet.train_model()
>>> 
>>>   File "/mimer/NOBACKUP/groups/deepmechalvis/andres/XAI_TurbulentChannel_optimized/main_code/py_bin/py_class/ann_config.py", line 629, in train_model
>>>     data_training = self.model.fit(train_data,batch_size=self.batch_size,verbose=2,
>>> 
>>>   File "/apps/Arch/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/utils/traceback_utils.py", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File "/apps/Arch/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/engine/training.py", line 1216, in fit
>>>     tmp_logs = self.train_function(iterator)
>>> 
>>>   File "/apps/Arch/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/engine/training.py", line 878, in train_function
>>>     return step_function(self, iterator)
>>> 
>>>   File "/apps/Arch/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/engine/training.py", line 866, in step_function
>>>     data = next(iterator)
>>> 
srun: error: alvis4-23: task 1: Exited with exit code 1
